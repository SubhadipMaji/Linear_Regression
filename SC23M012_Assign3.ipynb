{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dataframe for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filename):\n",
    "    data = pd.read_csv(filename+\".csv\", header=None)\n",
    "    #naming the columns\n",
    "    names = ['x'+str(col+1) for col in data.columns[:-1]] + ['y']\n",
    "    data.columns = names\n",
    "    dataset = data.copy()\n",
    "    #adding the x0 column to the dataset\n",
    "    dataset['x0'] = 1\n",
    "    dataset = dataset[['x0'] + list(dataset.columns[:-1])]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HoldOut Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_splitTrainTest(df_data, division_ratio = 0.7):\n",
    "    '''splitting the data into training and testing set'''\n",
    "    df_data = df_data.sample(frac = 1).reset_index(drop=True)\n",
    "    training_number = int(division_ratio * len(df_data))\n",
    "    train_x = df_data[:training_number][df_data.columns[:-1]]\n",
    "    train_y = df_data[:training_number][df_data.columns[-1]]\n",
    "\n",
    "    test_x = df_data[training_number:][df_data.columns[:-1]]\n",
    "    test_y = df_data[training_number:][df_data.columns[-1]]\n",
    "\n",
    "    train_y = np.matrix(train_y).reshape(-1,1)\n",
    "    test_y = np.matrix(test_y).reshape(-1,1)\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k- fold data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_splitTrainTest(df_data, folds = 5):\n",
    "    '''splitting the data into training and testing set'''\n",
    "    df_data = df_data.sample(frac = 1).reset_index(drop=True)\n",
    "    fold_size = int(len(df_data)/folds)\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    for i in range(folds):\n",
    "        if i == 0:\n",
    "            train_x = df_data[fold_size:][df_data.columns[:-1]]\n",
    "            train_y = df_data[fold_size:][df_data.columns[-1]]\n",
    "\n",
    "            test_x = df_data[:fold_size][df_data.columns[:-1]]\n",
    "            test_y = df_data[:fold_size][df_data.columns[-1]]\n",
    "        else:\n",
    "            train_x = pd.concat([train_x, df_data[i*fold_size:][:fold_size][df_data.columns[:-1]]])\n",
    "            train_y = pd.concat([train_y, df_data[i*fold_size:][:fold_size][df_data.columns[-1]]])\n",
    "\n",
    "            test_x = pd.concat([test_x, df_data[i*fold_size:][:fold_size][df_data.columns[:-1]]])\n",
    "            test_y = pd.concat([test_y, df_data[i*fold_size:][:fold_size][df_data.columns[-1]]])\n",
    "\n",
    "    train_y = np.matrix(train_y).reshape(-1,1)\n",
    "    test_y = np.matrix(test_y).reshape(-1,1)\n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(train_x, test_x):\n",
    "    '''normalising the data'''\n",
    "    for col in train_x.columns[1:]:\n",
    "        minimum = train_x[col].min()\n",
    "        diff = train_x[col].max() - minimum\n",
    "\n",
    "        train_x[col] = (train_x[col] - minimum) / diff \n",
    "        test_x[col] = (test_x[col] - minimum) / diff\n",
    "\n",
    "    return train_x, test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Square Method and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Least_Square(x, y, w):\n",
    "    '''calculating the least square error --> Cost Function'''\n",
    "    # y = np.matrix(y)\n",
    "    y_pred = np.matmul(x,w)\n",
    "    error = np.sum(np.square(y_pred - y))/2\n",
    "    return error\n",
    "\n",
    "def rmse(y, ypred):\n",
    "    '''calculating the mean squared error'''\n",
    "    return np.sqrt(np.mean(np.square(y - ypred))/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(train_x, train_y, alpha, epsilon):\n",
    "    '''batch gradient descent'''\n",
    "    w_init = np.zeros(train_x.shape[1]).reshape(-1,1)\n",
    "    train_x = np.matrix(train_x)\n",
    "    w_h = []\n",
    "    cost_list = []\n",
    "    w_old = w_init + 1\n",
    "\n",
    "    while np.linalg.norm(w_old - w_init) > epsilon:\n",
    "        w_old = w_init\n",
    "        w_init = w_init - alpha * np.matmul(train_x.T, np.matmul(train_x, w_init) - train_y)\n",
    "        w_h.append(w_init)\n",
    "        cost_list.append(Least_Square(train_x, train_y, w_init))\n",
    "\n",
    "    return w_init, w_h, cost_list\n",
    "\n",
    "def stochastic_gradient_descent(train_x, train_y, alpha, epsilon):\n",
    "    '''stochastic gradient descent'''\n",
    "    w_init = np.zeros(train_x.shape[1]).reshape(-1,1)\n",
    "    train_x = np.matrix(train_x)\n",
    "    w_h = []\n",
    "    cost_list = []\n",
    "    w_old = w_init + 1\n",
    "\n",
    "    while np.linalg.norm(w_old - w_init) > epsilon:\n",
    "        w_old = w_init\n",
    "        for i in range(len(train_x)):\n",
    "            w_init = w_init - alpha * np.matmul(train_x[i].T, np.matmul(train_x[i], w_init) - train_y[i])\n",
    "            w_h.append(w_init)\n",
    "            cost_list.append(Least_Square(train_x, train_y, w_init))\n",
    "\n",
    "    return w_init, w_h, cost_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyze Data 4 using linear regression\n",
    "\n",
    "(a) Describe the experimental procedures used.\n",
    "\n",
    "(b) Report the values of the parameters of the model.\n",
    "\n",
    "(c) Analyze the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Describe the experimental procedures used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Experimental procedure : \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Experimental procedure : \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in dataset4 is : 100 \n",
      "number of columns in dataset4 is : 202\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x192</th>\n",
       "      <th>x193</th>\n",
       "      <th>x194</th>\n",
       "      <th>x195</th>\n",
       "      <th>x196</th>\n",
       "      <th>x197</th>\n",
       "      <th>x198</th>\n",
       "      <th>x199</th>\n",
       "      <th>x200</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.003779</td>\n",
       "      <td>0.031027</td>\n",
       "      <td>-0.388220</td>\n",
       "      <td>1.420800</td>\n",
       "      <td>-0.78641</td>\n",
       "      <td>0.984240</td>\n",
       "      <td>0.75169</td>\n",
       "      <td>-1.158100</td>\n",
       "      <td>-0.55794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.62419</td>\n",
       "      <td>-0.84155</td>\n",
       "      <td>-0.797580</td>\n",
       "      <td>1.87690</td>\n",
       "      <td>-2.00010</td>\n",
       "      <td>-0.19268</td>\n",
       "      <td>1.638600</td>\n",
       "      <td>-0.590150</td>\n",
       "      <td>-0.42054</td>\n",
       "      <td>-839.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.036043</td>\n",
       "      <td>-1.159100</td>\n",
       "      <td>0.219710</td>\n",
       "      <td>-0.952310</td>\n",
       "      <td>-0.50804</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>1.12300</td>\n",
       "      <td>1.269000</td>\n",
       "      <td>2.18280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77773</td>\n",
       "      <td>-1.75710</td>\n",
       "      <td>1.144500</td>\n",
       "      <td>1.01170</td>\n",
       "      <td>-0.31249</td>\n",
       "      <td>0.40022</td>\n",
       "      <td>-1.059200</td>\n",
       "      <td>-0.454850</td>\n",
       "      <td>-0.34847</td>\n",
       "      <td>62.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.438100</td>\n",
       "      <td>-0.733720</td>\n",
       "      <td>-0.089075</td>\n",
       "      <td>1.220100</td>\n",
       "      <td>-1.40850</td>\n",
       "      <td>0.971640</td>\n",
       "      <td>1.13360</td>\n",
       "      <td>-0.406290</td>\n",
       "      <td>-0.64220</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.12920</td>\n",
       "      <td>-0.69191</td>\n",
       "      <td>-1.894400</td>\n",
       "      <td>-0.42760</td>\n",
       "      <td>0.33931</td>\n",
       "      <td>-0.72165</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.268480</td>\n",
       "      <td>0.24492</td>\n",
       "      <td>115.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.572800</td>\n",
       "      <td>-0.251010</td>\n",
       "      <td>-1.052400</td>\n",
       "      <td>0.377940</td>\n",
       "      <td>1.46930</td>\n",
       "      <td>-0.099663</td>\n",
       "      <td>2.48980</td>\n",
       "      <td>-0.041293</td>\n",
       "      <td>0.99387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24356</td>\n",
       "      <td>-1.07830</td>\n",
       "      <td>-1.690300</td>\n",
       "      <td>0.79401</td>\n",
       "      <td>0.32061</td>\n",
       "      <td>-0.35921</td>\n",
       "      <td>0.333220</td>\n",
       "      <td>1.396900</td>\n",
       "      <td>0.86721</td>\n",
       "      <td>101.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.129100</td>\n",
       "      <td>1.506500</td>\n",
       "      <td>0.532350</td>\n",
       "      <td>-0.760570</td>\n",
       "      <td>-1.67030</td>\n",
       "      <td>-0.714710</td>\n",
       "      <td>1.87340</td>\n",
       "      <td>-0.842890</td>\n",
       "      <td>0.33552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66104</td>\n",
       "      <td>0.94924</td>\n",
       "      <td>1.885600</td>\n",
       "      <td>-1.78200</td>\n",
       "      <td>0.76805</td>\n",
       "      <td>1.90530</td>\n",
       "      <td>1.018900</td>\n",
       "      <td>-0.854000</td>\n",
       "      <td>-0.17766</td>\n",
       "      <td>171.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.213600</td>\n",
       "      <td>2.387900</td>\n",
       "      <td>-1.217300</td>\n",
       "      <td>0.535470</td>\n",
       "      <td>-1.48810</td>\n",
       "      <td>-0.909300</td>\n",
       "      <td>-0.28999</td>\n",
       "      <td>-0.248500</td>\n",
       "      <td>0.49530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.24503</td>\n",
       "      <td>-0.32660</td>\n",
       "      <td>0.018761</td>\n",
       "      <td>0.22156</td>\n",
       "      <td>-0.65720</td>\n",
       "      <td>-0.39293</td>\n",
       "      <td>0.078999</td>\n",
       "      <td>0.762460</td>\n",
       "      <td>-1.55510</td>\n",
       "      <td>-533.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.546690</td>\n",
       "      <td>-0.756010</td>\n",
       "      <td>-1.358100</td>\n",
       "      <td>-1.363200</td>\n",
       "      <td>-0.86046</td>\n",
       "      <td>0.418790</td>\n",
       "      <td>0.56376</td>\n",
       "      <td>-0.309220</td>\n",
       "      <td>-0.95458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.28748</td>\n",
       "      <td>0.33814</td>\n",
       "      <td>-1.194600</td>\n",
       "      <td>-0.13763</td>\n",
       "      <td>0.95873</td>\n",
       "      <td>0.11038</td>\n",
       "      <td>-0.008607</td>\n",
       "      <td>0.248750</td>\n",
       "      <td>0.12928</td>\n",
       "      <td>-415.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>2.286900</td>\n",
       "      <td>1.505600</td>\n",
       "      <td>-1.845500</td>\n",
       "      <td>1.927000</td>\n",
       "      <td>-0.62383</td>\n",
       "      <td>-0.412170</td>\n",
       "      <td>-0.41538</td>\n",
       "      <td>1.095500</td>\n",
       "      <td>0.30687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.59265</td>\n",
       "      <td>-0.87901</td>\n",
       "      <td>-0.457040</td>\n",
       "      <td>0.17610</td>\n",
       "      <td>-1.75290</td>\n",
       "      <td>3.12710</td>\n",
       "      <td>-1.419700</td>\n",
       "      <td>0.073631</td>\n",
       "      <td>-1.25840</td>\n",
       "      <td>604.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>0.818020</td>\n",
       "      <td>1.714200</td>\n",
       "      <td>0.507840</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>-0.42719</td>\n",
       "      <td>0.455540</td>\n",
       "      <td>1.51450</td>\n",
       "      <td>0.540690</td>\n",
       "      <td>-2.12760</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.04040</td>\n",
       "      <td>-1.05690</td>\n",
       "      <td>-0.510540</td>\n",
       "      <td>-0.39106</td>\n",
       "      <td>-0.36509</td>\n",
       "      <td>-0.45803</td>\n",
       "      <td>-0.270980</td>\n",
       "      <td>-0.819550</td>\n",
       "      <td>-0.55253</td>\n",
       "      <td>148.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.347680</td>\n",
       "      <td>-1.525300</td>\n",
       "      <td>1.323000</td>\n",
       "      <td>-1.233700</td>\n",
       "      <td>-1.12500</td>\n",
       "      <td>-0.542260</td>\n",
       "      <td>-0.39169</td>\n",
       "      <td>-1.734400</td>\n",
       "      <td>-1.48900</td>\n",
       "      <td>...</td>\n",
       "      <td>1.09350</td>\n",
       "      <td>-1.35080</td>\n",
       "      <td>0.953670</td>\n",
       "      <td>-0.43802</td>\n",
       "      <td>-1.18650</td>\n",
       "      <td>0.96310</td>\n",
       "      <td>0.932960</td>\n",
       "      <td>1.915800</td>\n",
       "      <td>-0.55015</td>\n",
       "      <td>43.442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    x0        x1        x2        x3        x4       x5        x6       x7  \\\n",
       "0    1 -0.003779  0.031027 -0.388220  1.420800 -0.78641  0.984240  0.75169   \n",
       "1    1 -0.036043 -1.159100  0.219710 -0.952310 -0.50804  0.067726  1.12300   \n",
       "2    1 -0.438100 -0.733720 -0.089075  1.220100 -1.40850  0.971640  1.13360   \n",
       "3    1 -0.572800 -0.251010 -1.052400  0.377940  1.46930 -0.099663  2.48980   \n",
       "4    1 -0.129100  1.506500  0.532350 -0.760570 -1.67030 -0.714710  1.87340   \n",
       "..  ..       ...       ...       ...       ...      ...       ...      ...   \n",
       "95   1 -1.213600  2.387900 -1.217300  0.535470 -1.48810 -0.909300 -0.28999   \n",
       "96   1 -0.546690 -0.756010 -1.358100 -1.363200 -0.86046  0.418790  0.56376   \n",
       "97   1  2.286900  1.505600 -1.845500  1.927000 -0.62383 -0.412170 -0.41538   \n",
       "98   1  0.818020  1.714200  0.507840  0.010488 -0.42719  0.455540  1.51450   \n",
       "99   1 -0.347680 -1.525300  1.323000 -1.233700 -1.12500 -0.542260 -0.39169   \n",
       "\n",
       "          x8       x9  ...     x192     x193      x194     x195     x196  \\\n",
       "0  -1.158100 -0.55794  ... -0.62419 -0.84155 -0.797580  1.87690 -2.00010   \n",
       "1   1.269000  2.18280  ...  0.77773 -1.75710  1.144500  1.01170 -0.31249   \n",
       "2  -0.406290 -0.64220  ... -1.12920 -0.69191 -1.894400 -0.42760  0.33931   \n",
       "3  -0.041293  0.99387  ...  0.24356 -1.07830 -1.690300  0.79401  0.32061   \n",
       "4  -0.842890  0.33552  ... -0.66104  0.94924  1.885600 -1.78200  0.76805   \n",
       "..       ...      ...  ...      ...      ...       ...      ...      ...   \n",
       "95 -0.248500  0.49530  ... -0.24503 -0.32660  0.018761  0.22156 -0.65720   \n",
       "96 -0.309220 -0.95458  ... -0.28748  0.33814 -1.194600 -0.13763  0.95873   \n",
       "97  1.095500  0.30687  ... -0.59265 -0.87901 -0.457040  0.17610 -1.75290   \n",
       "98  0.540690 -2.12760  ... -1.04040 -1.05690 -0.510540 -0.39106 -0.36509   \n",
       "99 -1.734400 -1.48900  ...  1.09350 -1.35080  0.953670 -0.43802 -1.18650   \n",
       "\n",
       "       x197      x198      x199     x200        y  \n",
       "0  -0.19268  1.638600 -0.590150 -0.42054 -839.430  \n",
       "1   0.40022 -1.059200 -0.454850 -0.34847   62.063  \n",
       "2  -0.72165  0.226230  0.268480  0.24492  115.990  \n",
       "3  -0.35921  0.333220  1.396900  0.86721  101.340  \n",
       "4   1.90530  1.018900 -0.854000 -0.17766  171.260  \n",
       "..      ...       ...       ...      ...      ...  \n",
       "95 -0.39293  0.078999  0.762460 -1.55510 -533.230  \n",
       "96  0.11038 -0.008607  0.248750  0.12928 -415.730  \n",
       "97  3.12710 -1.419700  0.073631 -1.25840  604.320  \n",
       "98 -0.45803 -0.270980 -0.819550 -0.55253  148.450  \n",
       "99  0.96310  0.932960  1.915800 -0.55015   43.442  \n",
       "\n",
       "[100 rows x 202 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"data4\"\n",
    "dataset4 = create_dataset(filename)\n",
    "\n",
    "'''Visualize the dataset-4'''\n",
    "print(f\"number of rows in dataset4 is : {dataset4.shape[0]} \\nnumber of columns in dataset4 is : {dataset4.shape[1]}\")\n",
    "dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(dataset, mode='train', train_max=None, train_min=None):\n",
    "    '''normalizing the train and test dataset'''\n",
    "    data = dataset.copy()\n",
    "    if mode=='train':\n",
    "        train_max={}\n",
    "        train_min={}\n",
    "        for col in data.columns[1:]:\n",
    "            train_max[col] = data[col].max()\n",
    "            train_min[col] = data[col].min()\n",
    "            data[col] = (data[col] - train_min[col]) / (train_max[col] - train_min[col])\n",
    "        return data, train_min, train_max\n",
    "    \n",
    "    elif mode == 'test':\n",
    "        if train_min is None or train_max is None:\n",
    "            raise Exception('Pass train_min and/or train_max.')\n",
    "        for col in data.columns[1:]:\n",
    "            data[col] = (data[col] - train_min[col]) / (train_max[col] - train_min[col])\n",
    "        return data\n",
    "    \n",
    "\n",
    "\n",
    "def get_rmse(pred, y):\n",
    "    '''\n",
    "    Calculate root mean squared error.\n",
    "    '''\n",
    "    return np.sqrt(np.mean(np.square(pred - y))/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Report the values of the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of the parameters w: \n",
      "[matrix([[-305.81691139,  148.22884648, -153.37592053,   91.61977841,\n",
      "           72.43180942,   38.44187618,  -27.39588838,   36.71607837,\n",
      "          155.10699927,  -63.09663909, -116.28043984,  292.5565367 ,\n",
      "          -11.50145664,   77.81301568,   22.63923564, -150.3214022 ,\n",
      "          -46.32828118, -301.56975777, -233.52689723, -194.91055108,\n",
      "          -68.14276161,  255.54372129, -363.20915147,  -17.71197863,\n",
      "          238.13007525,  103.09952567, -113.31799063, -182.29296638,\n",
      "         -184.7659447 ,  -24.92898833,   34.53320905,  171.55188699,\n",
      "         -172.07481029,  -89.24585402,  278.04465385,  231.0016185 ,\n",
      "          -68.06533621,  206.08088802,   90.75634987,   68.49660694,\n",
      "          165.96628001,  110.90359992,  170.79951893,  -21.79217687,\n",
      "            3.57031383,   49.36520974,   60.40462952,  -29.00043873,\n",
      "           77.37185068,  147.58605497,   -3.36119338, -135.80938899,\n",
      "         -153.84174332,  314.83251536,  -76.49524208,    6.51450491,\n",
      "         -273.60259009, -214.755663  ,  -44.17427097, -160.15855444,\n",
      "           90.72132882,  196.80662455,   33.03708044, -143.92993588,\n",
      "         -216.4412379 , -415.62485527, -154.43541932,  -41.73110627,\n",
      "           43.69341167,   89.48004995,  -25.43007067,   22.68668521,\n",
      "           24.87720955,  166.96120997,  -45.85455888,   -4.77168412,\n",
      "          135.95766449,  -82.64487419, -101.48551007,  114.88252535,\n",
      "         -141.05234123,  122.23859415,  101.98088205, -167.12351806,\n",
      "          -49.82269039,   68.05532008, -317.53059617,   35.18669652,\n",
      "          169.55088931,  -40.94657167, -176.49417016, -306.81144053,\n",
      "          -22.83224148, -241.38302598,   -5.32909337,  252.09190099,\n",
      "          -47.64138878,   59.01769565, -305.8793741 ,  -55.21911477,\n",
      "         -243.34792387,  429.1536814 , -134.37249407,  207.7711654 ,\n",
      "          -47.51284926, -185.7492102 , -126.46668829,   46.63097291,\n",
      "          -39.36026389, -259.2569482 , -187.73942121,  -86.35080408,\n",
      "         -101.9543952 ,  175.92747427,  388.90791454,  170.30708453,\n",
      "          228.71158306,  -78.45009512,   18.64100201,  -88.24954126,\n",
      "           15.63508003,   59.29872316,  268.41091194,  -35.03584141,\n",
      "          -39.82189231,   29.54487837,    1.28113417,   45.2945499 ,\n",
      "           31.47964138,  161.20046853,   29.79075401,  -69.83848405,\n",
      "           82.41335092,  127.89624444,  283.78975749,  -14.69606663,\n",
      "          172.76848754,  -65.80289533,  171.37492456,  -25.21568492,\n",
      "          123.19680626,   12.3595794 ,  263.33537641, -160.9539605 ,\n",
      "           44.55923907,  -70.12977976,   71.94483166,  -88.76217901,\n",
      "           34.99710451,   86.01077324, -129.93375108,  118.68254784,\n",
      "          -89.35639392,  -70.99670561,  -47.95333582,   42.33367619,\n",
      "         -129.30554688,   88.22322104,  225.19713781,  -10.04693335,\n",
      "         -121.82959146,  300.72909809,  -31.74734534,   40.20782355,\n",
      "          -30.29443349,  -85.44824612,  -55.07827767,  146.10971797,\n",
      "          282.46019904, -158.05114294, -247.0410614 ,  -11.12322998,\n",
      "          159.72218763,  101.61436366,  -42.88069104,  145.70856915,\n",
      "            2.68938539, -133.87211811,   34.03036851,   35.1091808 ,\n",
      "         -126.28146376,   32.65432675,  236.50090821,  -24.48764862,\n",
      "           24.47658228,  126.54428735, -212.04692969,  160.54713502,\n",
      "          166.2768648 ,  -92.60444328, -115.96218524,  -76.222953  ,\n",
      "          256.79648028,  -69.78846095,  -78.72527295,  -88.35181302,\n",
      "          -15.58646536, -127.30701314,   99.00082995,    3.53148219,\n",
      "          -64.0120519 ]])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_x = dataset4[dataset4.columns[:-1]]\n",
    "train_y = dataset4[dataset4.columns[-1]]\n",
    "train_x, min_value, max_value = normalize_data(train_x, mode='train')\n",
    "\n",
    "'''using Lagarangian Multiplier Method to calculate w'''\n",
    "x = np.matrix(train_x)\n",
    "y = np.matrix(train_y)\n",
    "y.shape\n",
    "w = np.matmul(np.matmul(x.T, np.linalg.inv(np.matmul(x, x.T))), y.T)\n",
    "\n",
    "print(f\"value of the parameters w: \\n{list(w.T)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Analyze the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of the model ---> RMSE : 614.5892503702254\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.matmul(x, w)\n",
    "print(f\"Performance of the model ---> RMSE : {get_rmse(y_pred, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analyze Computer hardware data set using linear regression (download data from UCI web repository)\n",
    "\n",
    "(a) Analyze the data with normalization and without normalization.\n",
    "\n",
    "(b) Describe how you applied normalization techniques on training and testing data.\n",
    "\n",
    "(c) Apply random subsampling and k fold cross validation.\n",
    "\n",
    "(d) Assess the performance of the model.\n",
    "\n",
    "(e) Report the values of the hyperparameters and the parameters of the \n",
    "model.\n",
    "\n",
    "(f) Apply batch as well as online optimization algorithms and compare \n",
    "their performance in terms of time and MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x0     x1      x2       x3     x4    x5     x6    y\n",
       "0  1.0  125.0   256.0   6000.0  256.0  16.0  128.0  198\n",
       "1  1.0   29.0  8000.0  32000.0   32.0   8.0   32.0  269\n",
       "2  1.0   29.0  8000.0  32000.0   32.0   8.0   32.0  220\n",
       "3  1.0   29.0  8000.0  32000.0   32.0   8.0   32.0  172\n",
       "4  1.0   29.0  8000.0  16000.0   32.0   8.0   16.0  132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'computer+hardware/machine.data', names=['vendor_name', 'model_name', 'MYCT', 'MMIN', 'MMAX', 'CACH', 'CHMIN', 'CHMAX', 'PRP', 'ERP'])\n",
    "data = data.drop(['vendor_name', 'model_name', 'ERP'], axis=1)\n",
    "\n",
    "#saving the data to a csv file\n",
    "data.to_csv('modified_Uci.csv', index=False, header=False)\n",
    "\n",
    "filename = \"modified_Uci\"\n",
    "dataset = create_dataset(filename)\n",
    "\n",
    "#converting values of each column of the daataset to float\n",
    "for col in dataset.columns[:-1]:\n",
    "    dataset[col] = dataset[col].astype(float)  \n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Analyze the data with normalization and without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data without normalization: \n",
      "      x0     x1       x2       x3     x4    x5    x6\n",
      "0    1.0  185.0   2000.0  16000.0   16.0   1.0   6.0\n",
      "1    1.0  240.0    512.0   2000.0    8.0   1.0   5.0\n",
      "2    1.0  140.0   2000.0  32000.0   32.0   1.0  54.0\n",
      "3    1.0  133.0   1000.0  12000.0    9.0   3.0  12.0\n",
      "4    1.0  400.0   2000.0   4000.0    0.0   1.0   1.0\n",
      "..   ...    ...      ...      ...    ...   ...   ...\n",
      "141  1.0  112.0   1000.0   1000.0    0.0   1.0   4.0\n",
      "142  1.0   30.0  16000.0  32000.0  256.0  16.0  24.0\n",
      "143  1.0   50.0   2000.0   8000.0    8.0   1.0   5.0\n",
      "144  1.0   50.0   2000.0  16000.0    8.0   3.0   5.0\n",
      "145  1.0   70.0   4000.0  12000.0    8.0   6.0   8.0\n",
      "\n",
      "[146 rows x 7 columns]\n",
      "..................................................\n",
      "train data with normalization: \n",
      "      x0        x1        x2        x3        x4        x5        x6\n",
      "0    1.0  0.028995  0.246988  0.249249  0.125000  0.019231  0.034091\n",
      "1    1.0  0.078220  0.058735  0.186687  0.035156  0.057692  0.068182\n",
      "2    1.0  0.527984  0.012048  0.124124  0.000000  0.019231  0.022727\n",
      "3    1.0  0.393122  0.044177  0.030280  0.000000  0.019231  0.005682\n",
      "4    1.0  0.022252  0.121486  0.249249  0.093750  0.019231  0.034091\n",
      "..   ...       ...       ...       ...       ...       ...       ...\n",
      "141  1.0  0.140256  0.121486  0.061562  0.031250  0.057692  0.034091\n",
      "142  1.0  0.211059  0.058735  0.030280  0.000000  0.019231  0.011364\n",
      "143  1.0  0.008766  0.497992  1.000000  0.375000  0.230769  1.000000\n",
      "144  1.0  0.008092  0.497992  0.499499  0.125000  0.153846  0.181818\n",
      "145  1.0  0.109912  0.012425  0.061562  0.000000  0.019231  0.017045\n",
      "\n",
      "[146 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#analyzing the data without normalization\n",
    "train_x, train_y, test_x, test_y = holdout_splitTrainTest(dataset, 0.7)\n",
    "\n",
    "#analyzing the data with normalization\n",
    "train_x_normal, train_y_normal, test_x_normal, test_y_normal = holdout_splitTrainTest(dataset, 0.7)\n",
    "train_x_normal, test_x_normal = normalise(train_x_normal, test_x_normal)\n",
    "\n",
    "print(f\"train data without normalization: \\n{train_x}\")\n",
    "print(\"..................................................\")\n",
    "print(f\"train data with normalization: \\n{train_x_normal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Describe how you applied normalization techniques on training and testing data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "normalizing technique used in the dataset described below:\n",
    "  min-max normalization :\n",
    "     x = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "The normalisation technique used here is minmax for the training data. Then the weights are trained over it. Using this normalised training data\n",
    "the optimal weights are deduced from stochastic gradient descent method. Then the testing set is\n",
    "normalised using the maxima and minima from the training set. This normalised test set is then\n",
    "used to predict the new y. This y is used to check the performance measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Apply random subsampling and k fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monteCarloCrossvalidation(k, dataset):\n",
    "    '''monte carlo cross validation'''\n",
    "    alpha_list = [0.01, 0.002, 0.03, 0.001, 0.674]\n",
    "    rmse_list = []\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    for i in range(k):\n",
    "        rmse_list.clear()\n",
    "        for alpha in alpha_list:  \n",
    "            #performiong holdout split\n",
    "            train_x, train_y, test_x, test_y = holdout_splitTrainTest(dataset, 0.7)\n",
    "            \n",
    "            #normalizing the train and test data\n",
    "            train_x, test_x = normalise(train_x, test_x)\n",
    "\n",
    "            #performing schoastic gradient descent on training data\n",
    "            w, w_h, j_w = stochastic_gradient_descent(train_x, train_y, alpha, epsilon)\n",
    "\n",
    "            #predicting the test data\n",
    "            test_x = np.matrix(test_x)\n",
    "            y_pred = np.matmul(test_x,w)\n",
    "            rmse_error = rmse(test_y, y_pred)\n",
    "            print(f\"alpha : {alpha}, rmse_error : {rmse_error}\")\n",
    "            rmse_list.append(rmse_error)\n",
    "\n",
    "        #plotting the rmse vs alpha graph\n",
    "        plt.scatter(alpha_list, rmse_list, marker='x', color='r')\n",
    "        plt.plot(alpha_list, rmse_list, color='b')\n",
    "        plt.xlabel('alpha')\n",
    "        plt.ylabel('rmse')\n",
    "        plt.show()\n",
    "        print(\"---------------------------------------------------\")\n",
    "\n",
    "    return 0\n",
    "\n",
    "# k-fold cross validation\n",
    "def kfold_cross_validation(k, dataset):\n",
    "    '''k-fold cross validation'''\n",
    "    alpha_list = [0.01, 0.002, 0.03, 0.001, 0.674]\n",
    "    rmse_list = []\n",
    "    # splitting the data into k folds   \n",
    "    train_x, train_y, test_x, test_y = kfold_splitTrainTest(dataset, k)\n",
    "    #normalizing the train and test data\n",
    "    train_x, test_x = normalise(train_x, test_x)\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for alpha in alpha_list:  \n",
    "        #performing schoastic gradient descent on training data\n",
    "        w, w_h, j_w = stochastic_gradient_descent(train_x, train_y, alpha, epsilon)\n",
    "\n",
    "        #predicting the test data\n",
    "        test_x = np.matrix(test_x)\n",
    "        y_pred = np.matmul(test_x,w)\n",
    "        rmse_error = rmse(test_y, y_pred)\n",
    "        print(f\"alpha : {alpha}, rmse_error : {rmse_error}\")\n",
    "        rmse_list.append(rmse_error)\n",
    "\n",
    "    #plotting the rmse vs alpha graph\n",
    "    plt.scatter(alpha_list, rmse_list, marker='x', color='r')\n",
    "    plt.plot(alpha_list, rmse_list, color='b')\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('rmse')\n",
    "    plt.show()\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying monte carlo cross validation on dataset\n",
    "# monteCarloCrossvalidation(1, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold_cross_validation(5, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Assess the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of the model ---> RMSE : 9.125816244154345\n"
     ]
    }
   ],
   "source": [
    "w, w_h, j_w = stochastic_gradient_descent(train_x_normal, train_y_normal, 0.01, 1e-3)\n",
    "y_pred = np.matmul(test_x_normal, w)\n",
    "print(f\"Performance of the model ---> RMSE : {get_rmse(y_pred, test_y_normal)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Report the values of the hyperparameters and the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hyperparameters are : alpha = 0.01, epsilon = 1e-6\n",
      "The parameters are : \n",
      "[[-51.12118467  56.93876624 177.98500364 434.84994731 129.54877799\n",
      "  -72.95888564 311.5070667 ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The hyperparameters are : alpha = 0.01, epsilon = 1e-6\")\n",
    "print(f\"The parameters are : \\n{w.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Apply batch as well as online optimization algorithms and compare their perfor-\n",
    "mance in terms of time and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of the model ---> RMSE : 9.13308191126496\n"
     ]
    }
   ],
   "source": [
    "w_b, w_b_h, j_w_b = batch_gradient_descent(train_x_normal, train_y_normal, 0.0001, 1e-9)\n",
    "y_pred_b = np.matmul(test_x_normal, w_b)\n",
    "print(f\"Performance of the model ---> RMSE : {get_rmse(y_pred_b, test_y_normal)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
